{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "noeNSubpv4tI",
        "oJFZv8WIv8nX",
        "TdOQhYUYwBZR",
        "_AfVmCTPwzC6",
        "hJg_51hSA-k6"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This file is for fine tuning the model and some data exploration. The model has already been fine-tuned and is pushed to the huggingface hub. It can be used without running this again."
      ],
      "metadata": {
        "id": "SKUkYUxYBJFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library requirements"
      ],
      "metadata": {
        "id": "noeNSubpv4tI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGQyyrZsv2kC"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging in to HuggingFace"
      ],
      "metadata": {
        "id": "oJFZv8WIv8nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "nRcjeddzwBB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration and splitting"
      ],
      "metadata": {
        "id": "TdOQhYUYwBZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder, load_dataset, get_dataset_split_names, get_dataset_config_names"
      ],
      "metadata": {
        "id": "iTtf2vKdwOdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the dataset object first to see info about the data\n",
        "dataset_object = load_dataset_builder(\"mlqa\", \"mlqa.en.en\")"
      ],
      "metadata": {
        "id": "7HFGTr_QwS7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all information about the dataset\n",
        "dataset_object.info"
      ],
      "metadata": {
        "id": "kBj2Fb3KwZ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get specific description and features from dataset\n",
        "display(dataset_object.info.description)\n",
        "print(\"\\n\")\n",
        "display(dataset_object.info.features)"
      ],
      "metadata": {
        "id": "M_KnIOcGwbmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then loading the entire dataset\n",
        "full_dataset = load_dataset(\"mlqa\", \"mlqa.en.en\")"
      ],
      "metadata": {
        "id": "hO_pCxyCwdfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the splits of the dataset\n",
        "display(get_dataset_split_names(\"mlqa\", \"mlqa.en.en\"))"
      ],
      "metadata": {
        "id": "Cy6hD5z8wfCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the test split in the original data already has around 11.5k instances,\n",
        "# so we're just going to divide that into 3 datasets for train, val, and test\n",
        "\n",
        "# splitting that into train and test set\n",
        "split_dataset_traintest = full_dataset[\"test\"].train_test_split(test_size=0.3, seed=42)\n",
        "split_dataset_traintest"
      ],
      "metadata": {
        "id": "VCVd96gywuxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, splitting the new train set into train and val set\n",
        "split_dataset_trainval = split_dataset_traintest[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
        "split_dataset_trainval"
      ],
      "metadata": {
        "id": "lDA9YBBzwwmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "_AfVmCTPwzC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "ZCmNB_Olw4O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the model and tokenizer from hugging face\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/bert-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"huggingface-course/bert-finetuned-squad\")"
      ],
      "metadata": {
        "id": "s4jOIPQhw55E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function from huggingface docs- https://huggingface.co/docs/transformers/tasks/question_answering\n",
        "\n",
        "def preprocess_function(all_data):\n",
        "  questions = [q.strip() for q in all_data[\"question\"]]\n",
        "  inputs = tokenizer(\n",
        "      questions,\n",
        "      all_data[\"context\"],\n",
        "      max_length=384,\n",
        "      truncation=\"only_second\",\n",
        "      return_offsets_mapping=True,\n",
        "      padding=\"max_length\",\n",
        "  )\n",
        "\n",
        "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "  answers = all_data[\"answers\"]\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "\n",
        "  for i, offset in enumerate(offset_mapping):\n",
        "    answer = answers[i]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "      idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "      idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label it (0, 0)\n",
        "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "      start_positions.append(0)\n",
        "      end_positions.append(0)\n",
        "    else:\n",
        "    # Otherwise it's the start and end token positions\n",
        "      idx = context_start\n",
        "      while idx <= context_end and offset[idx][0] <= start_char:\n",
        "        idx += 1\n",
        "      start_positions.append(idx - 1)\n",
        "\n",
        "      idx = context_end\n",
        "      while idx >= context_start and offset[idx][1] >= end_char:\n",
        "        idx -= 1\n",
        "      end_positions.append(idx + 1)\n",
        "\n",
        "  inputs[\"start_positions\"] = start_positions\n",
        "  inputs[\"end_positions\"] = end_positions\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "Py2nsdyVxCNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the train, val, and test sets\n",
        "\n",
        "tokenized_dataset_train = split_dataset_trainval[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_dataset_val = split_dataset_trainval[\"test\"].map(preprocess_function, batched=True)\n",
        "tokenized_dataset_test = split_dataset_traintest[\"test\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "OTtvlvctxF2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(tokenized_dataset_train)\n",
        "display(tokenized_dataset_val)\n",
        "display(tokenized_dataset_test)"
      ],
      "metadata": {
        "id": "H6Ff5TTIxHtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "data_collator = DefaultDataCollator()"
      ],
      "metadata": {
        "id": "xabCjixSxKp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the hyperparameters, making a trainer object, and training the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"diff_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3QDM1m00xMin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting train log history, losses per epoch for training and validation\n",
        "\n",
        "display(trainer.state.log_history)\n",
        "\n",
        "text = 'These are losses per epoch for training and validation'\n",
        "with open('diff_training_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\" + str(trainer.state.log_history)+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "2so8dJcNQzHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_loss = trainer.evaluate()\n",
        "display(overall_loss)\n",
        "\n",
        "text = 'This is the overall evaluation loss on validation set'\n",
        "with open('diff_training_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\" + str(overall_loss)+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "ayLumpx0VTFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(model.eval())\n",
        "\n",
        "text = 'This is the model evaluation info'\n",
        "with open('diff_training_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\" + str(model.eval())+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "ZAHFaeysTizl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"diff_training_results.csv\")"
      ],
      "metadata": {
        "id": "IpixLoCdvJx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload model to hub"
      ],
      "metadata": {
        "id": "hJg_51hSA-k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pushing the model to hub, can directly access it this pre-trained model fine-tuned on our dataset\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "8RDgoo7ixXQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}