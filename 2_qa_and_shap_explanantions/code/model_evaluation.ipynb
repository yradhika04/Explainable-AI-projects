{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1dUegNgDfvC8",
        "J-sVc-t4ftHR",
        "Ja-BfiV8jHw1",
        "-NyMBKVYYSuA",
        "kum587WxXleA",
        "DyfO4otxZEyK",
        "ekEy3w2rgPWO",
        "hmc8hn54pztY"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library requirements"
      ],
      "metadata": {
        "id": "1dUegNgDfvC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy1TIvpfXeY4"
      },
      "outputs": [],
      "source": [
        "! pip install datasets\n",
        "! pip install transformers\n",
        "! pip install accelerate -U\n",
        "! pip install evaluate\n",
        "! pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading and splitting data again"
      ],
      "metadata": {
        "id": "J-sVc-t4ftHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "KBwrbUJ3cjhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the entire dataset\n",
        "full_dataset = load_dataset(\"mlqa\", \"mlqa.en.en\")"
      ],
      "metadata": {
        "id": "T0BIwTNLfEwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to split into train and test set, same seed as other file\n",
        "split_dataset_traintest = full_dataset[\"test\"].train_test_split(test_size=0.3, seed=42)\n",
        "split_dataset_traintest"
      ],
      "metadata": {
        "id": "BOYYDn7ChKLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split previous train set into train and val set\n",
        "split_dataset_trainval = split_dataset_traintest[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
        "split_dataset_trainval"
      ],
      "metadata": {
        "id": "uaDIRqvGlj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "Ja-BfiV8jHw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "NvgscmH2ENdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "V6W_pg5jr6ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting our fine-tuned model and its tokenizer now\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"radyad/diff_model\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"radyad/diff_model\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "oKS_TOVMEItf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "testset = split_dataset_traintest[\"test\"]\n",
        "all_predicted_answers = []\n",
        "shap_data = [] # questions+context for shap\n",
        "predictions = [] # for formatted entries of model's answers for evalution on squad\n",
        "truth = [] # for formatted entries of actual answer's for evalution on squad\n",
        "only_predictions = [] # for evaluation on bleu, meteor etc.\n",
        "only_truth = [] # for evaluation on bleu, meteor etc.\n",
        "start_scores = []\n",
        "end_scores = []\n",
        "tokens = []\n",
        "\n",
        "for i in tqdm(range(len(testset))):\n",
        "  # special_context = re.sub('[^a-zA-Z0-9.()]+', ' ', testset[i]['context'])\n",
        "  '''\n",
        "  text = testset[i]['context']\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in stopwords.words(\"english\")]\n",
        "  text= \" \".join(text)\n",
        "  '''\n",
        "\n",
        "  if len(testset[i]['context'])>1600:\n",
        "    context = testset[i]['context'][:1600]\n",
        "  else:\n",
        "    context = testset[i]['context']\n",
        "\n",
        "  inputs = tokenizer(testset[i]['question'], context, return_tensors=\"pt\")\n",
        "  inputs.to(device)\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "  input_for_tokens = tokenizer.encode(testset[i]['question'], context)\n",
        "  tokens.append(tokenizer.convert_ids_to_tokens(input_for_tokens))\n",
        "  start_scores.append(outputs.start_logits)\n",
        "  end_scores.append(outputs.end_logits)\n",
        "\n",
        "  answer_start_index = outputs.start_logits.argmax()\n",
        "  answer_end_index = outputs.end_logits.argmax()\n",
        "  predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "  each_predicted_answer = tokenizer.decode(predict_answer_tokens)\n",
        "  all_predicted_answers.append(each_predicted_answer)\n",
        "  print(each_predicted_answer)\n",
        "\n",
        "  # a list of questions+context in the shap format for using it later\n",
        "  each_data = [testset[i]['question']+\"[SEP]\"+context]\n",
        "  shap_data.append(each_data)\n",
        "\n",
        "  # putting predictions and actual answers in the format for evaluating on squad dataset\n",
        "  no_answer_probability = 1 if len(testset[i]['answers']['answer_start'])!=0 else 0\n",
        "  each_formatted_pred = {'prediction_text':each_predicted_answer, 'id': testset[i]['id']}\n",
        "  each_formatted_truth = {'answers': testset[i]['answers'], 'id': testset[i]['id']}\n",
        "  predictions.append(each_formatted_pred)\n",
        "  truth.append(each_formatted_truth)\n",
        "\n",
        "  # putting predictions and actual answers in the format for calculating bleu score\n",
        "  only_predictions.append(each_predicted_answer)\n",
        "  only_truth.append(testset[i]['answers']['text'][0])\n"
      ],
      "metadata": {
        "id": "eXtQx4AzHwEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating sqaud metrics scores: exact match and f1\n",
        "from evaluate import load\n",
        "squad_metric = load(\"squad\")\n",
        "results = squad_metric.compute(predictions=predictions, references=truth)\n",
        "display(results)\n",
        "\n",
        "text = 'These are the exact-match and f1 scores for the test data'\n",
        "with open('diff_evaluation_results.csv','w') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\"+str(results)+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "G1J-dFyOBbQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating meteor scores\n",
        "meteor = load(\"meteor\")\n",
        "results = meteor.compute(predictions=only_predictions, references=only_truth)\n",
        "display(results)\n",
        "\n",
        "text = 'This is the meteor score for the test data'\n",
        "with open('diff_evaluation_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\"+str(results)+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "TpQ0EQZqd3j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating bleu scores\n",
        "bleu = load(\"bleu\")\n",
        "results = bleu.compute(predictions=only_predictions, references=only_truth)\n",
        "display(results)\n",
        "\n",
        "text = 'These are the bleu scores for the test data'\n",
        "with open('diff_evaluation_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\"+str(results)+\"\\n\")\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "fuWHq-cHazqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating google bleu scores\n",
        "google_bleu = load(\"google_bleu\")\n",
        "results = google_bleu.compute(predictions=only_predictions, references=only_truth)\n",
        "display(results)\n",
        "\n",
        "text = 'This is the google-bleu score for the test data'\n",
        "with open('diff_evaluation_results.csv','a') as f:\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\"+str(results)+\"\\n\")"
      ],
      "metadata": {
        "id": "FTM6GlR4k1pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"diff_evaluation_results.csv\")"
      ],
      "metadata": {
        "id": "ttOzV09omCHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer explanations using SHAP"
      ],
      "metadata": {
        "id": "-NyMBKVYYSuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get 6 random samples for generating their shap plots\n",
        "import random\n",
        "random.seed(446)\n",
        "\n",
        "random_list = random.sample(range(1, len(testset)), 6)\n",
        "random_list"
      ],
      "metadata": {
        "id": "PdeH3G_IWfY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_list = [333, 3146, 2486, 3225, 1954, 3136]"
      ],
      "metadata": {
        "id": "BgH5SnOoFF9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_list)):\n",
        "  display(only_predictions[random_list[i]])\n",
        "  display(only_truth[random_list[i]])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "EK8rFXupGdHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import shap\n",
        "import torch\n",
        "\n",
        "# load the model\n",
        "pmodel = transformers.pipeline('question-answering', model=\"radyad/diff_model\")\n",
        "\n",
        "# define two predictions, one that outputs the logits for the range start,\n",
        "# and the other for the range end\n",
        "def f(questions, start):\n",
        "    outs = []\n",
        "    for q in questions:\n",
        "        question, context = q.split(\"[SEP]\")\n",
        "        d = pmodel.tokenizer(question, context)\n",
        "        out = pmodel.model.forward(**{k: torch.tensor(d[k]).reshape(1, -1) for k in d})\n",
        "        logits = out.start_logits if start else out.end_logits\n",
        "        outs.append(logits.reshape(-1).detach().numpy())\n",
        "    return outs\n",
        "def f_start(questions):\n",
        "    return f(questions, True)\n",
        "def f_end(questions):\n",
        "    return f(questions, False)\n",
        "\n",
        "# attach a dynamic output_names property to the models so we can plot the tokens at each output position\n",
        "def out_names(inputs):\n",
        "    question, context = inputs.split(\"[SEP]\")\n",
        "    d = pmodel.tokenizer(question, context)\n",
        "    return [pmodel.tokenizer.decode([id]) for id in d[\"input_ids\"]]\n",
        "\n",
        "f_start.output_names = out_names\n",
        "f_end.output_names = out_names"
      ],
      "metadata": {
        "id": "v0YUPb4tlgoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explainer objects for start and end positions\n",
        "\n",
        "explainer_start = shap.Explainer(f_start, pmodel.tokenizer)\n",
        "explainer_end = shap.Explainer(f_end, pmodel.tokenizer)"
      ],
      "metadata": {
        "id": "4cBq_nQg7rqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start positions SHAP plot"
      ],
      "metadata": {
        "id": "kum587WxXleA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_shap_values_start = []\n",
        "\n",
        "for i in range(len(random_list)):\n",
        "  shap_values_start = explainer_start(shap_data[random_list[i]])\n",
        "  all_shap_values_start.append(shap_values_start)\n",
        "  plot = shap.plots.text(shap_values_start, display=False)\n",
        "  filename = \"sample\" + str(i+1) +\"_start\"\n",
        "  file = open(filename + \".html\",'w')\n",
        "  file.write(plot)\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "sOoy_Hr20dRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_list)):\n",
        "  filename = \"sample\" + str(i+1) +\"_start\"\n",
        "  files.download(filename + \".html\")"
      ],
      "metadata": {
        "id": "gDnR8ZhL9JXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End positions SHAP plot"
      ],
      "metadata": {
        "id": "DyfO4otxZEyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only for the answers with more than one word in the answer\n",
        "\n",
        "all_shap_values_end = []\n",
        "answers_with_end = []\n",
        "\n",
        "for i in range(len(random_list)):\n",
        "  words = only_predictions[random_list[i]].split()\n",
        "  if(len(words))>1:\n",
        "    answers_with_end.append(i)\n",
        "    shap_values_end = explainer_end(shap_data[random_list[i]])\n",
        "    all_shap_values_end.append(shap_values_end)\n",
        "    plot = shap.plots.text(shap_values_end, display=False)\n",
        "    filename = \"sample\" + str(i+1) +\"_end\"\n",
        "    file = open(filename + \".html\",'w')\n",
        "    file.write(plot)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "qMaNrv6_1Vxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(answers_with_end)):\n",
        "  filename = \"sample\" + str(answers_with_end[i]+1) +\"_end\"\n",
        "  files.download(filename + \".html\")"
      ],
      "metadata": {
        "id": "sIYllO5srGaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start scores bar graph"
      ],
      "metadata": {
        "id": "ekEy3w2rgPWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "plt.rcParams[\"figure.figsize\"] = (10,8)"
      ],
      "metadata": {
        "id": "RQB3rnuujh7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_list)):\n",
        "  # getting the start scores\n",
        "  index = random_list[i]\n",
        "  s_scores = start_scores[index].detach().cpu().numpy().flatten()\n",
        "\n",
        "  # adding token index to each token label for the plot\n",
        "  token_labels = []\n",
        "  for (j, token) in enumerate(tokens[index]):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, j))\n",
        "\n",
        "  # putting start scores and tokens into a dataframe and choosing the top 20\n",
        "  start_scores_and_token_labels = list(zip(s_scores,token_labels))\n",
        "  df = pd.DataFrame(start_scores_and_token_labels, columns=['start_scores','token_labels'])\n",
        "  df = df.nlargest(20, \"start_scores\")\n",
        "\n",
        "  # plotting the top 20 start scores\n",
        "  ax = sns.barplot(x=df[\"token_labels\"], y=df[\"start_scores\"], errorbar=None)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "  ax.grid(True)\n",
        "  plt.title('Start scores for top 20 tokens')\n",
        "  filename= \"top20_start_sample\" + str(i+1) + \".png\"\n",
        "  plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "  plt.clf()\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "r5zFs2J8jia7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(random_list)):\n",
        "  filename= \"top20_start_sample\" + str(i+1) + \".png\"\n",
        "  files.download(filename)"
      ],
      "metadata": {
        "id": "BBHLGDRVqnC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End scores bar graph"
      ],
      "metadata": {
        "id": "hmc8hn54pztY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(answers_with_end)):\n",
        "  # getting the end scores\n",
        "  temp_index = answers_with_end[i]\n",
        "  index = random_list[temp_index]\n",
        "  e_scores = end_scores[index].detach().cpu().numpy().flatten()\n",
        "\n",
        "  # adding token index to each token label for the plot\n",
        "  token_labels = []\n",
        "  for (j, token) in enumerate(tokens[index]):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, j))\n",
        "\n",
        "  # putting end scores and tokens into a dataframe and choosing the top 20\n",
        "  end_scores_and_token_labels = list(zip(e_scores,token_labels))\n",
        "  df = pd.DataFrame(end_scores_and_token_labels, columns=['end_scores','token_labels'])\n",
        "  df = df.nlargest(20, \"end_scores\")\n",
        "\n",
        "  # plotting the top 20 end scores\n",
        "  ax = sns.barplot(x=df[\"token_labels\"], y=df[\"end_scores\"], errorbar=None)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "  ax.grid(False)\n",
        "  plt.title('End scores for top 20 tokens')\n",
        "  filename= \"top20_end_sample\" + str(temp_index+1) + \".png\"\n",
        "  plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "  plt.clf()\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "VYxoGOVRp5dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(answers_with_end)):\n",
        "  filename= \"top20_end_sample\" + str(answers_with_end[i]+1) + \".png\"\n",
        "  files.download(filename)"
      ],
      "metadata": {
        "id": "1mRjmXPStZ13"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}